# Recommender Systems
> #### _Archit, Shen, Shrey | Fall '22 | AIPI 590 Take Home Challange_
&nbsp;

## About the Project
lorem ipsum lorem ipsum lorem ipsumlorem ipsumlorem ipsumlorem ipsumlorem ipsumlorem ipsumlorem ipsumlorem ipsumlorem ipsumlorem ipsumlorem ipsumlorem ipsumlorem ipsumlorem ipsum

&nbsp;
## Datasets overview
lorem ipsumlorem ipsumlorem ipsumlorem ipsumlorem ipsumlorem ipsumlorem ipsumlorem ipsumlorem ipsumlorem ipsumlorem ipsum

**1. Dataset #1 - Retail Rocket Dataset**
lorem ipsumlorem ipsumlorem ipsumlorem ipsumlorem ipsumlorem ipsumlorem ipsumlorem ipsumlorem ipsumlorem ipsumlorem ipsum

**2. Dataset #1 - H&M Dataste**
lorem ipsumlorem ipsumlorem ipsumlorem ipsumlorem ipsumlorem ipsumlorem ipsumlorem ipsumlorem ipsumlorem ipsumlorem ipsum

&nbsp;
## Steps to run the code

The Jupyter notebook at  `Dataset_1_Retail_Rocket/RR_SA2C_Recommender.ipynb` contains all the code and corresponding descriptions to reproduce the results. For your convenience, the steps are recapped below:

1. Launch `RR_SA2C_Recommender.ipynb` in a Google Colab instance.
2. Run the first cell to clone the git repository containing all source code.
3. Run the second cell to install required Python library.
4. Run the third cell to download the Retail Rocket dataset to the Colab instance.
5. Run the fourth cell to pre-process data and generate replay buffer for Deep Reinforcement Learning.
6. Run the final cell to begin model training and evaluation.

&nbsp;
## Evaluation Metrics & Results
The evaluation metrics used are Normalized Discounted Cumulative Gain (NDCG), Mean Reciprocal Rank (MRR), Mean Average Precision (MAP), and Hit Ratio (HR).

- NDCG@k measures the quality of the recommendation list based on the top-k ranking of items in the list higher ranked items are scored higher
- RR@K' ("reciprocal-rank-at-k"): is the inverse rank (one divided by the rank) of the first item among the top-K recommended that is in the test data. The average across users is typically referred to as the "Mean Reciprocal Rank" or MRR
- map@K measures the average precision@K averaged over all queries (for the entire dataset)
- HR@k measures whether the ground-truth item is in the top-k positions of the recommendation list generated by the model

&nbsp;

**1. Results for Retail Rocket Dataset**
|DRL Model Results|Non DRL Model Results|
|--|--|
|<table> <tr><th>NDCG@10</th><th>HR@10</th></tr><tr><td>0.5180</td><td>0.6250</td></tr> </table>|<table> <tr><th>NDCG@10</th><th>MRR@10</th><th>MAP@10</th></tr><tr><td>0.0007</td><td>0.0015</td><td>0.0015<td></tr> </table>|

**2. Results for H&M Dataset**
|DRL Model Results|Non DRL Model Results|
|--|--|
|<table> <tr><th>NDCG@10</th><th>HR@10</th></tr><tr><td>0.5180</td><td>0.6250</td></tr> </table>|<table> <tr><th>NDCG@10</th><th>MRR@10</th><th>MAP@10</th></tr><tr><td>0.0014</td><td>0.0026</td><td>0.0026<td></tr> </table>|

&nbsp;
# Folder structure

```
ðŸ“¦recommenders_aipi590
â”£ ðŸ“‚DRL_Recommenders
 â”ƒ â”£ ðŸ“‚Dataset_1_Retail_Rocket
 â”ƒ â”ƒ â”£ ðŸ“‚RR_data
 â”ƒ â”ƒ â”ƒ â”— ðŸ“œREADME.md
 â”ƒ â”ƒ â”£ ðŸ“‚src
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œNextItNetModules_v2.py
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œSA2C_v2.py
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œSASRecModules_v2.py
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œgen_replay_buffer.py
 â”ƒ â”ƒ â”ƒ â”— ðŸ“œutility_v2.py
 â”ƒ â”ƒ â”£ ðŸ“œRR_SA2C_Recommender.ipynb
 â”ƒ â”ƒ â”— ðŸ“œrequirements.txt
 â”ƒ â”£ ðŸ“‚Dataset_2_HM
 â”ƒ â”ƒ â”£ ðŸ“‚HM_data
 â”ƒ â”ƒ â”ƒ â”— ðŸ“œREADME.md
 â”ƒ â”ƒ â”£ ðŸ“‚src
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œNextItNetModules_v2.py
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œSASRecModules_v2.py
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œSNQN_v2.py
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“œgen_replay_buffer.py
 â”ƒ â”ƒ â”ƒ â”— ðŸ“œutility_v2.py
 â”ƒ â”ƒ â”£ ðŸ“œHM_SNQN_Recommender.ipynb
 â”ƒ â”ƒ â”£ ðŸ“œHM_SNQN_SASRec.ipynb
 â”ƒ â”ƒ â”— ðŸ“œrequirements.txt
 â”ƒ â”— ðŸ“œREADME.md
 â”£ ðŸ“‚Non_DRL_Recommenders
 â”ƒ â”£ ðŸ“‚Dataset_1_Retail_Rocket
 â”ƒ â”ƒ â”— ðŸ“œBPR_Retail_Rocket.ipynb
 â”ƒ â”£ ðŸ“‚Dataset_2_HM
 â”ƒ â”ƒ â”— ðŸ“œBPR_HM.ipynb
 â”ƒ â”£ ðŸ“œREADME.md
 â”ƒ â”£ ðŸ“œbpr_model.py
 â”ƒ â”— ðŸ“œrequirements.txt
 â”£ ðŸ“œ.gitignore
 â”— ðŸ“œREADME.md
```