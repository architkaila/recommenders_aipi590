# Deep Reinforcement Learning Recommenders

There are two different DRL models in this section - each trained on a different dataset. The first one was trained on the Retail Rocket dataset, and the second one was trained on the H&M dataset. Both models were implemented using the code shared by Xin Xin et al in their paper entitled "Supervised Advantage Actor-Critic for Recommender Systems" [1].


# Dataset #1 - Retail Rocket

## Dataset overview

The first dataset used was from Retail Rocket [2]. Retail Rocket is a company that generates personalized product recommendations for shopping websites and provides customer segmentation based on user interests and other parameters. The dataset was collected from a real-world e-commerce website and consists of raw data, i.e. data without any content transformation. However, all values are hashed to address confidentiality concerns. Among the files in the dataset, only the behavior data (_events.csv_) is used in this project. The behavior data is a timestamped log of events like clicks, add to carts, and transactions that represent different interactions made by visitors on the e-commerce website over a time period of 4.5 months. There are a total of 2756101 events produced by 1407580 unique visitors. 

## Data preparation

The data from _events.csv_ is subjected to some cleaning in preparation for modeling. First, to simplify the event types, all records marked as 'transaction' are removed and the `transid` column is dropped. And to maintain comparability with Dataset #2, all 'addtocart' events are treated as purchases. Next, all items and users with less than 3 interactions are dropped. For ease of modeling, the session ids, item ids, and event type are encoded as integers using LabelEncoder. The behavior column (i.e. event type) is replaced by a new column with binary values indicating whether an item was purchased or not. This pre-processed data is saved as a CSV file and pickled as a data frame for subsequent use.

The session ids are randomly shuffled and split into three sets - train, validation, and test - where the ratio is 80%, 10%, and 10% respectively. The corresponding data is then allocated to one of the three sets according to their session id. This split data is also pickled as data frames for subsequent use in the model.

The popularity of items in the data set is calculated using a simple method: the more frequent it appears in the data set, the higher its popularity score. The popularity scores are normalized over the total number of records in the data set.

Finally, the replay buffer is created from the train data set. The replay buffer consists of an input sequence (representing state), an item id (representing action), another sequence (representing next state), a binary flag for whether the item is added to cart, and a done flag. Each sequence is composed of 10 interacted items. Sequences with a length less than 10 are complemented with a padding item. All these information are stored in a dictionary and pickled as a data frame. Upon passing a custom flag to the  `Dataset_1_Retail_Rocket/src/gen_replay_buffer` script, the replay buffer data can also be saved to a CSV file for manual exploration purpose. Overall information like the sequence size and number of unique items are also saved to a data frame.

## Model

The recommendation model used for this data set is SASRec-SA2C. SASRec is a sequential recommendation model reliant on the self-attention mechanism. It is based on the Transformer architecture and its output is treated as the latent sequence state. SA2C stands for Supervised Advantage Actor-Critic and is one of the common reinforcement learning algorithms. The basic idea in SA2C is to measure how much "advantage" could be obtained by taking an action as compared to the average scenario. The "advantage" helps to identify actions leading to high cumulative rewards and assign higher weights to them. SA2C is integrated with SASRec to form a Deep Reinforcement Learning model.

The Adam optimizer is used for training the model. And the learning rate used is 0.001. The embedding size (i.e. number of hidden factors) is set to 64. The number of heads for self attention in SASRec is set to 1. The batch size is 256.

The model is trained using the train data set and evaluated with the validation data set after every 8000 batches. At the end of training, the model is evaluated with the test data set.

## Results

The evaluation metrics used are Hit Ratio (HR) and Normalized Discounted Cumulative Gain (NDCG). HR@k measures whether the ground-truth item is in the top-k positions of the recommendation list generated by the model, whereas NDCG@k measures the quality of the recommendation list based on the top-k ranking of items in the list - higher ranked items are scored higher.

To maintain comparability with Dataset #2, only **Purchase** recommendation results are shown below. All figures are rounded to 4 decimal places. NG is short for NDCG.

|Metric@k|HR@5|NG@5|HR@10|NG@10|HR@15|NG@15|HR@20|NG@20|
|--|--|--|--|--|--|--|--|--|
|Value|0.5842|0.5047|0.6250|0.5180|0.6435|0.5229|0.6578|0.5263|

## Steps to run the code

The Jupyter notebook at  `Dataset_1_Retail_Rocket/RR_SA2C_Recommender.ipynb` contains all the code and corresponding descriptions to reproduce the results. For your convenience, the steps are recapped below:

1. Launch `RR_SA2C_Recommender.ipynb` in a Google Colab instance.
2. Run the first cell to clone the git repository containing all source code.
3. Run the second cell to install required Python library.
4. Run the third cell to download the Retail Rocket dataset to the Colab instance.
5. Run the fourth cell to pre-process data and generate replay buffer for Deep Reinforcement Learning.
6. Run the final cell to begin model training and evaluation.

# Dataset #2 - H&M

## Dataset overview

## Data preparation

## Model

## Results

## Steps to run the code


# Folder structure

```
├── Dataset_1_Retail_Rocket
|   ├── RR_data
|   │   ├── README.md
|   ├── src
|   │   ├── NextItNetModules_v2.py
|   │   ├── SA2C_v2.py
|   │   ├── SASRecModules_v2.py
|   │   ├── gen_replay_buffer.py
|   │   └── utility_v2.py
│   ├── RR_SA2C_Recommender.ipynb
│   └── requirements.txt
├── Dataset_2_HM
|   ├── HM_data
|   │   ├── README.md
|   ├── src
|   │   ├── NextItNetModules_v2.py
|   │   ├── SASRecModules_v2.py
|   │   ├── SNQN_v2.py
|   │   ├── gen_replay_buffer.py
|   │   └── utility_v2.py
|   ├── HM_SNQN_Recommender.ipynb
|   ├── HM_SNQN_SASRec.ipynb
|   └── requirements.txt
└── README.md
```

# References

[1] X. Xin, A. Karatzoglou, I. Arapakis, and J. M. Jose, “Supervised Advantage Actor-Critic for Recommender Systems,”  _Proceedings of ACM Conference (Conference’17)_, 2021.

[2] Retailrocket, “Retailrocket Recommender System Dataset,”  _Kaggle_, 24-Mar-2017. [Online]. Available: https://www.kaggle.com/datasets/retailrocket/ecommerce-dataset. [Accessed: 5-Dec-2022].

[3] H&M Group, “H&M personalized fashion recommendations,”  _Kaggle_, 09-May-2022. [Online]. Available: https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations. [Accessed: 5-Dec-2022].
